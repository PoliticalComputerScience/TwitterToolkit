{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraping By Hashtag Script\n",
    "\n",
    "    demo with #pulwama\n",
    "    \n",
    "@Author: Robbie Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twitterscraper\n",
      "  Downloading https://files.pythonhosted.org/packages/38/7d/0bf84247b78d7d223914cbf410e1160203a65d39086aaf8c6cad521cec74/twitterscraper-0.9.3.tar.gz\n",
      "Collecting coala-utils~=0.5.0 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/00/74ec750cfc4e830f9d1cfdd4d559f3d2d4ba1b834b78d5266446db3fd1d6/coala_utils-0.5.1-py3-none-any.whl\n",
      "Collecting bs4 (from twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Collecting lxml (from twitterscraper)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/6c/436a534dca42f7982ba793983353035d117ab70541266704974efa323ade/lxml-4.3.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (8.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.7MB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/user/miniconda3/lib/python3.7/site-packages (from twitterscraper) (2.21.0)\n",
      "Collecting beautifulsoup4 (from bs4->twitterscraper)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/5d/3260694a59df0ec52f8b4883f5d23b130bc237602a1411fa670eae12351e/beautifulsoup4-4.7.1-py3-none-any.whl (94kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 21.9MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/user/miniconda3/lib/python3.7/site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/user/miniconda3/lib/python3.7/site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/user/miniconda3/lib/python3.7/site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/lib/python3.7/site-packages (from requests->twitterscraper) (2019.3.9)\n",
      "Collecting soupsieve>=1.2 (from beautifulsoup4->bs4->twitterscraper)\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/f8/e54b1d771ed4fab66b3fa1c178e137a3c73d84fb6f64329bddf0da5a371c/soupsieve-1.9-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: twitterscraper, bs4\n",
      "  Building wheel for twitterscraper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/user/Library/Caches/pip/wheels/45/50/9b/70128bca07e2bf8b5ed3f504002e9e74a6eaa5e756341b6931\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/user/Library/Caches/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built twitterscraper bs4\n",
      "Installing collected packages: coala-utils, soupsieve, beautifulsoup4, bs4, lxml, twitterscraper\n",
      "Successfully installed beautifulsoup4-4.7.1 bs4-0.0.1 coala-utils-0.5.1 lxml-4.3.3 soupsieve-1.9 twitterscraper-0.9.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import csv\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from twitterscraper import query_tweets\n",
    "except:\n",
    "    !{sys.executable} -m pip install twitterscraper\n",
    "    from twitterscraper import query_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracted Query\n",
    "\n",
    "Performs query, adds the hashtag field, writes to csv file with name output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_write(query, output_name, \n",
    "                      limit=None,\n",
    "                      begindate=dt.date(2006, 3, 21), \n",
    "                      enddate=dt.date.today(), \n",
    "                      poolsize=20, \n",
    "                      lang=''):\n",
    "    # perform query\n",
    "    tweets = query_tweets(query, limit, begindate, enddate, poolsize, lang)\n",
    "    # construct csv\n",
    "    if tweets:\n",
    "        with open(output_name, \"w\", encoding=\"utf-8\") as output:\n",
    "            f = csv.writer(output)\n",
    "            f.writerow([\"timestamp\", \"user\", \"fullname\", \"text\", \"hashtags\", \"id\", \"url\", \"retweets\", \"favorites\", \"replies\"])\n",
    "            for x in tweets:\n",
    "                # parse text for hashtags\n",
    "                tag_set = set(re.findall('\\#\\w+', x.text))\n",
    "                tag_values = \" \".join(tag_set)\n",
    "                # add row for tweet in csv\n",
    "                f.writerow([x.timestamp, x.user, x.fullname, x.text, tag_values, x.id, x.url, x.retweets, x.likes, x.replies])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['#pulwama since:2019-03-01 until:2019-03-02', '#pulwama since:2019-03-02 until:2019-03-03', '#pulwama since:2019-03-03 until:2019-03-04', '#pulwama since:2019-03-04 until:2019-03-05', '#pulwama since:2019-03-05 until:2019-03-06', '#pulwama since:2019-03-06 until:2019-03-07', '#pulwama since:2019-03-07 until:2019-03-08', '#pulwama since:2019-03-08 until:2019-03-09', '#pulwama since:2019-03-09 until:2019-03-10', '#pulwama since:2019-03-10 until:2019-03-11', '#pulwama since:2019-03-11 until:2019-03-12', '#pulwama since:2019-03-12 until:2019-03-13', '#pulwama since:2019-03-13 until:2019-03-14', '#pulwama since:2019-03-14 until:2019-03-15']\n",
      "INFO: Querying #pulwama since:2019-03-01 until:2019-03-02\n",
      "INFO: Querying #pulwama since:2019-03-02 until:2019-03-03\n",
      "INFO: Querying #pulwama since:2019-03-03 until:2019-03-04\n",
      "INFO: Querying #pulwama since:2019-03-05 until:2019-03-06\n",
      "INFO: Querying #pulwama since:2019-03-04 until:2019-03-05\n",
      "INFO: Querying #pulwama since:2019-03-06 until:2019-03-07\n",
      "INFO: Querying #pulwama since:2019-03-07 until:2019-03-08\n",
      "INFO: Querying #pulwama since:2019-03-08 until:2019-03-09\n",
      "INFO: Querying #pulwama since:2019-03-09 until:2019-03-10\n",
      "INFO: Querying #pulwama since:2019-03-10 until:2019-03-11\n",
      "INFO: Querying #pulwama since:2019-03-11 until:2019-03-12\n",
      "INFO: Querying #pulwama since:2019-03-14 until:2019-03-15\n",
      "INFO: Querying #pulwama since:2019-03-12 until:2019-03-13\n",
      "INFO: Querying #pulwama since:2019-03-13 until:2019-03-14\n",
      "INFO: Got 275 tweets for %23pulwama%20since%3A2019-03-12%20until%3A2019-03-13.\n",
      "INFO: Got 275 tweets (275 new).\n",
      "INFO: Got 279 tweets for %23pulwama%20since%3A2019-03-14%20until%3A2019-03-15.\n",
      "INFO: Got 554 tweets (279 new).\n",
      "INFO: Got 342 tweets for %23pulwama%20since%3A2019-03-09%20until%3A2019-03-10.\n",
      "INFO: Got 896 tweets (342 new).\n",
      "INFO: Got 334 tweets for %23pulwama%20since%3A2019-03-13%20until%3A2019-03-14.\n",
      "INFO: Got 1230 tweets (334 new).\n",
      "INFO: Got 357 tweets for %23pulwama%20since%3A2019-03-08%20until%3A2019-03-09.\n",
      "INFO: Got 1587 tweets (357 new).\n",
      "INFO: Got 397 tweets for %23pulwama%20since%3A2019-03-10%20until%3A2019-03-11.\n",
      "INFO: Got 1984 tweets (397 new).\n",
      "INFO: Got 488 tweets for %23pulwama%20since%3A2019-03-07%20until%3A2019-03-08.\n",
      "INFO: Got 2472 tweets (488 new).\n",
      "INFO: Got 565 tweets for %23pulwama%20since%3A2019-03-11%20until%3A2019-03-12.\n",
      "INFO: Got 3037 tweets (565 new).\n",
      "INFO: Got 578 tweets for %23pulwama%20since%3A2019-03-06%20until%3A2019-03-07.\n",
      "INFO: Got 3615 tweets (578 new).\n",
      "INFO: Got 658 tweets for %23pulwama%20since%3A2019-03-04%20until%3A2019-03-05.\n",
      "INFO: Got 4273 tweets (658 new).\n",
      "INFO: Got 849 tweets for %23pulwama%20since%3A2019-03-03%20until%3A2019-03-04.\n",
      "INFO: Got 5122 tweets (849 new).\n",
      "INFO: Got 1003 tweets for %23pulwama%20since%3A2019-03-05%20until%3A2019-03-06.\n",
      "INFO: Got 6125 tweets (1003 new).\n",
      "INFO: Got 1258 tweets for %23pulwama%20since%3A2019-03-01%20until%3A2019-03-02.\n",
      "INFO: Got 7383 tweets (1258 new).\n",
      "INFO: Got 1252 tweets for %23pulwama%20since%3A2019-03-02%20until%3A2019-03-03.\n",
      "INFO: Got 8635 tweets (1252 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "query_and_write(query='#pulwama', output_name='pulwama.csv', limit=None, \n",
    "             begindate=dt.date(2019, 3, 1), \n",
    "             enddate = dt.date(2019, 3, 15))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
